
---
title: "Data Science Methods - Assignment 2"
author:
- "M. Alberti, s.n. 2020162"
- "N. Ceschin, s.n. 344510"
header-includes:
  - \usepackage{bm}
date: March 16, 2020
output: pdf_document

---

## Question 1

First we upload all relevant libraries:

```{r results='hide', message=FALSE, warning=FALSE}
rm(list=ls())

# load libraries
library(leaps)
library(glmnet) # for lasso and ridge
library(Matrix)
library(pROC)
library(xtable)
```

Data preparation

```{r results='hide', message=FALSE, warning=FALSE}
setwd("C:/Users/nicol/Documents/Tilburg/Courses/Data Science Methods/DATA-SCIENCE-ASSIGNMENTS/Assignment 2")
#setwd("C:/Users/Mr Nobody/Desktop/Uni/EME/Data science Methods/Assignments/Assignment 2")

data<- read.csv("R_class.csv", sep=",", dec=".", header=TRUE)

ca <- grep("ca", names(data), value=T)  #vector of names of columns containing ca
drops <- names(data) %in% c(ca)
data <- data[!drops]  

# drop vars not used
assets <- grep("assets", names(data), value=T)
stocks <- grep("stocks", names(data), value=T)
narrowm <- grep("narrowm", names(data), value=T)
money <- grep("money", names(data), value=T)
ltrate <- grep("ltrate", names(data), value=T)
stir <- grep("stir", names(data), value=T)
loans <- grep("loans", names(data), value=T)
debt <- grep("debt", names(data), value=T)
er <- grep("er", names(data), value=T)
cpi <- grep("cpi", names(data), value=T)
gap <- grep("gap", names(data), value=T)
glo <- grep("a_", names(data), value=T)
gdp <- grep("gdp", names(data), value=T)
i <- grep("i_", names(data), value=T)
c <- grep("c_", names(data), value=T)
ri <- grep("ri", names(data), value=T)
rc <- grep("rc", names(data), value=T)

drops <- names(data) %in% c("year", "ccode", stocks, money, stir,assets,i,ri,glo) 
saves <- names(data) %in% c(glo)
full <- data[!drops] # drops those variables which have true indication in "drops"
full <- cbind(data[glo], full)

# FULL SET: omit observations with missing values
full_om <- na.omit(full)


#DATA for logit model
## interaction-terms for logit model
ia_pub<-data$pdebt_gap*data$ltrate
data$ia_pub<-ia_pub

ia_prb<-data$loans1_y_gap*data$ltrate
data$ia_prb<-ia_prb

ia_jb<-data$loans1_y_gap*data$ltrate*data$pdebt_gap
data$ia_jb<-ia_jb

ia_lygr<-data$loans1_y*data$gr_rgdp
data$ia_lygr<-ia_lygr

ia_pygr<-data$pdebt*data$gr_rgdp
data$ia_pygr<-ia_pygr

ia_lyer<-data$loans1_y_gap*data$er_gap
data$ia_lyer<-ia_lyer

## country factor
data$country.factor<-as.factor(data$ccode)

#throw out vars not used
drops.logit <- names(data) %in% c("year") # true-false indicator: true at the names in vector
full.logit <- data[!drops] # drops those variables which have true indication in "drops"

```

Preparation before running the estimation: 

```{r results='hide', message=FALSE, warning=FALSE}
# variables
var.list <- c( "loans1_y_gap", "pdebt_gap", "narrowm_y_gap",  "rltrate", "gr_rgdp", "gr_cpi",  "er_gap", "loans1_y", "pdebt", "ltrate")
# model list
model.list <- c("Ridge regression", "Lasso Regression")

# variables (logit)
var.logit <- c("loans1_y_gap", "pdebt_gap", "narrowm_y_gap",  "rltrate", "gr_rgdp", "gr_cpi",  "er_gap")
# interaction terms (logit)
ia.logit <- c("ia_pub", "ia_prb", "ia_jb", "ia_lygr", "ia_pygr", "ia_lyer")


# parameter list
param.list <- c("B", "$ J_{try} $", "$ J $", "\\# of crises")
out.list <- c("\\textbf{Model}", "AUC", "95\\%-CI", "N")

# miscellaneous non-independent
misc.list <- c("b2","b1","b3","rec1","rec2","rec3")

# table matrices
out <- matrix(nrow=2, ncol=5)

# Bootstrap runs
runs <- 100

# confidence intervals
n.ci <- 3
ci <- c(0.99, 0.95, 0.9)

```

Setting up the logit framework:

```{r results='hide', message=FALSE, warning=FALSE}
#LOGIT

aucs <- matrix(nrow=2, ncol=runs)
ci95_lo <- matrix(nrow=2, ncol=runs)
ci95_up <- matrix(nrow=2, ncol=runs)

N <- matrix(nrow=2, ncol=runs)
	

# get formula
location <- names(full.logit) %in% c(var.logit, ia.logit,"country.factor") # get location of vars
name <- names(full.logit[location]) # get names
indep <- paste(name, collapse="+") # indep. variables
dep <- paste("b2~") # dep. variable
fmla <- as.formula(paste(dep, indep)) # get formula

```

Ridge regression:

```{r results='hide', message=FALSE, warning=FALSE}
#drop alternative independent variables
misc.list <- c("b1","b3","rec1","rec2","rec3")
location <- names(full_om) %in% c(misc.list) 
name.indep <- names(full_om[!location])
indep <- full_om[name.indep]
dep <- factor(full_om[,"b2"]>0)

grid = 10 ^ seq(2, -3, length=100)

for(j in 1:runs) {
	
	# training, test sample
	set.seed(j)
	indexes = sample(1:nrow(full.logit), size=0.632*nrow(full.logit), replace=F)
	test = na.omit(full.logit[-indexes,])
	train = na.omit(full.logit[indexes,])
	
	# Ridge Regression
	
	train.mat = model.matrix(fmla, data=train)
  test.mat = model.matrix(fmla, data=test)
  mod.ridge = cv.glmnet(train.mat, train[, "b2"], alpha=0, lambda=grid, family="binomial", thresh=1e-12) 
  lambda.best = mod.ridge$lambda.min 
  
  # OOS-analysis
  pred = predict(mod.ridge,test.mat, s=lambda.best, type="response") # predicted outcome

  N[1,j] <- mod.ridge[["glmnet.fit"]][["nobs"]]
  
  location <- names(test) %in% c("b2")
	name <- names(test[location]) # get names
	true<-test[,name] # real outcome

	r<-roc(true,as.numeric(pred),ci=T) # ROC analysis
	
	aucs[1,j] <- as.numeric(r$auc)
		
	ci95_lo[1,j] <- as.numeric(ci.auc(r,conf.level=ci[2]))[1]
	ci95_up[1,j] <- as.numeric(ci.auc(r,conf.level=ci[2]))[3]

}

n_ridge <- trunc(as.numeric(colMeans(as.matrix(N[1, ])))) # update output table matrix

auc_ridge<-as.numeric(colMeans(as.matrix(aucs[1, ])))
ci95_lo_ridge<-as.numeric(colMeans(as.matrix(ci95_lo[1, ])))
ci95_up_ridge<-as.numeric(colMeans(as.matrix(ci95_up[1, ])))

out[1,1]<-model.list[1]
out[1,2]<-auc_ridge
out[1,3]<-ci95_lo_ridge
out[1,4]<-ci95_up_ridge
out[1,5]<-n_ridge

```

Lasso Regression 

```{r results='hide', warning=FALSE, message=FALSE}

for(j in 1:runs) {
	
	# training, test sample
	set.seed(j+100)
	indexes = sample(1:nrow(full.logit), size=0.632*nrow(full.logit), replace=F)
	test = na.omit(full.logit[-indexes,])
	train = na.omit(full.logit[indexes,])
	
	# Ridge Regression
	
	train.mat = model.matrix(fmla, data=train)
  test.mat = model.matrix(fmla, data=test)
  mod.lasso = cv.glmnet(train.mat, train[, "b2"], alpha=1, lambda=grid, family="binomial", thresh=1e-12) 
  lambda.best = mod.lasso$lambda.min 
  
  # OOS-analysis
  pred = predict(mod.lasso,test.mat, s=lambda.best, type="response") # predicted outcome

  N[2,j] <- mod.lasso[["glmnet.fit"]][["nobs"]]
  
  location <- names(test) %in% c("b2")
	name <- names(test[location]) # get names
	true<-test[,name] # real outcome

	r<-roc(true,pred,ci=T) # ROC analysis
	
	aucs[2,j] <- as.numeric(r$auc)
		
	ci95_lo[2,j] <- as.numeric(ci.auc(r,conf.level=ci[2]))[1]
	ci95_up[2,j] <- as.numeric(ci.auc(r,conf.level=ci[2]))[3]

}

# update output table matrix

n_lasso <- trunc(as.numeric(colMeans(as.matrix(N[2, ])))) 
auc_lasso<-as.numeric(colMeans(as.matrix(aucs[2, ])))
ci95_lo_lasso<-as.numeric(colMeans(as.matrix(ci95_lo[2, ])))
ci95_up_lasso<-as.numeric(colMeans(as.matrix(ci95_up[2, ])))

out[2,1]<-model.list[2]
out[2,2]<-auc_lasso
out[2,3]<-ci95_lo_lasso
out[2,4]<-ci95_up_lasso
out[2,5]<-n_lasso


```

Output table:

```{r}
#OUTPUT TABLE (always use double the amount of backslashes needed in latex)

# reformat decimals
out2<-out
out2[,2:5]<-round(as.numeric(out2[,2:5]), digits=2)

# confidence intervals
cis<-paste(out2[,3], out2[,4], sep=",")
cis<-paste("[", cis, sep="")
cis<-paste(cis, "]", sep="")

out3 <- out2[,c(1,2,5)] # leave out .9, .99 lower-ci columns
out4 <- cbind(out3,cis)
out5 <- out4[,c(1,2,4,3)] 
outF<-rbind(out.list,out5) #get model headers

# get rid of row and columnnames
x <- data.frame(outF)
outF<-as.matrix(x)
rownames(outF) <- rep("", nrow(outF))
colnames(outF) <- rep("", ncol(outF))

mat3<-xtable(outF, align="ccccc", caption="Comparison of Lasso and Ridge regression", label="tab:output") 

print(mat3, type="latex", caption.placement="top", hline.after=c(-1,nrow(mat3)), sanitize.text.function = function(x){x}, replace=T, floating=F, booktabs=T, include.colnames=F, include.rownames=F)

```

##Question 2
# a) 
Plain lasso optimization problem
$$  \hat{\bm \beta_{L}} =\arg\min_{\bm \beta}\left[ \Vert\bm y-\sum_{j=1}^{p} \bm x_j \beta_j \Vert^2 + \lambda\sum_{j=1}^{p}|\beta_j|\right]$$
Adaptive lasso optimization problem
$$  \hat{\bm \beta_{AL}} =\arg\min_{\bm \beta}\left[ \Vert\bm y-\sum_{j=1}^{p} \bm x_j \beta_j \Vert^2 + \lambda\sum_{j=1}^{p}|\beta_j|w_j\right]$$
with $w_i=\frac{1}{|\hat\beta_P|^{\gamma}}$, $\gamma\in\{1,2\}, \bm\beta_P$ pre-estimator.
Plain lasso is a less parsimonious model, it is suggested when we are interested in out-of-sample performance of our model (prediction). 
On the contrary, adaptive lasso is recommended when we are interested in the in-sample properties of the estimated parameters. The weight $w_j$ is inversely proportional to the magnitude of the coefficients in the preestimation: higher coefficients get penalized less in adaptive lasso. Therefore, coefficients that are smaller in the pre-estimation are potentially shrunk to zero. Adaptive lasso is therefore more parsimonious than plain lasso. Coefficients that would be close to zero in plain lasso are probably biased and in adaptive lasso they get penalized further and shrunk to zero. Under mild conditions, this method can select the true underlying model.
The tuning parameters are usually selected from the cross-validated MSE. In plain lasso, there is one tuning paramter, $\lambda$, while in the adaptive lasso there are two, $\gamma and \lambda$, so the cross-validation will be two-dimensional. Even if it does not have sound statistical foundations, sometimes it is suggested to pick not the $\lambda$ that minimizes the cross-validated MSE, but the one  that is one standard error above it.


# b)
The most intuitive reason to apply other estimation techniques after having found the true model by adaptive lasso (under some regularity conditions) is that shrinkage estimators are inherently biased. The penalty in the objective function ($\lambda$ above) enters the estimator formula, producing a bias. This bias tends to be high especially in finite samples. Therefore, it is reasonably better to apply adaptive lasso to find the covariates which characterize the true model and then apply an unbiased estimation technique on this restricted set of covariates.
Antoher possible reason we can think about could be that in some estimation techniques (as OLS) the higher the number of regressors, the higher the variance of the final estimator. Therefore, this would end up in higher standard errors and this could eventually inflate p-values. Thus, running estimation on a smaller set of covariates would yield a more precise estiamtors, with lower standard errors. However, the significance of finalcoefficients would potentially not be a problem, as we assume that adaptive lasso already selects just the covariates different from 0.












