
---
title: "Data Science Methods - Assignment 2"
author:
- "M. Alberti, s.n. 2020162"
- "N. Ceschin, s.n. 344510"
header-includes:
  - \usepackage{bm}
  - \usepackage{booktabs}
  - \usepackage{multirow}
  - \usepackage{booktabs}
  - \usepackage{longtable}
  - \usepackage{array}
  - \usepackage{multirow}
  - \usepackage{wrapfig}
  - \usepackage{float}
  - \usepackage{colortbl}
  - \usepackage{pdflscape}
  - \usepackage{tabu}
  - \usepackage{threeparttable}
  - \usepackage{threeparttablex}
  - \usepackage[normalem]{ulem}
  - \usepackage{makecell}
  - \usepackage{xcolor}
  - \usepackage{amsmath}

date: March 16, 2020
output: pdf_document

---

## Question 1

First we upload all relevant libraries:

```{r results='hide', message=FALSE, warning=FALSE}
rm(list=ls())

# load libraries
library(leaps)
library(glmnet) # for lasso and ridge
library(Matrix)
library(pROC)
library(xtable)
library(knitr)
library(kableExtra)
library(dplyr)
```

Data preparation

```{r results='hide', message=FALSE, warning=FALSE}
setwd("C:/Users/nicol/Documents/Tilburg/Courses/Data Science Methods/DATA-SCIENCE-ASSIGNMENTS/Assignment 2")
#setwd("C:/Users/Mr Nobody/Desktop/Uni/EME/Data science Methods/Assignments/Assignment 2")

data<- read.csv("R_class.csv", sep=",", dec=".", header=TRUE)

ca <- grep("ca", names(data), value=T)  #vector of names of columns containing ca
drops <- names(data) %in% c(ca)
data <- data[!drops]  

# drop vars not used
assets <- grep("assets", names(data), value=T)
stocks <- grep("stocks", names(data), value=T)
narrowm <- grep("narrowm", names(data), value=T)
money <- grep("money", names(data), value=T)
ltrate <- grep("ltrate", names(data), value=T)
stir <- grep("stir", names(data), value=T)
loans <- grep("loans", names(data), value=T)
debt <- grep("debt", names(data), value=T)
er <- grep("er", names(data), value=T)
cpi <- grep("cpi", names(data), value=T)
gap <- grep("gap", names(data), value=T)
glo <- grep("a_", names(data), value=T)
gdp <- grep("gdp", names(data), value=T)
i <- grep("i_", names(data), value=T)
c <- grep("c_", names(data), value=T)
ri <- grep("ri", names(data), value=T)
rc <- grep("rc", names(data), value=T)

drops <- names(data) %in% c("year", "ccode", stocks, money, stir,assets,i,ri,glo) 
saves <- names(data) %in% c(glo)
full <- data[!drops] # drops those variables which have true indication in "drops"
full <- cbind(data[glo], full)

# FULL SET: omit observations with missing values
full_om <- na.omit(full)
misc.list <- c("b1","b3","rec1","rec2","rec3")
location <- names(full_om) %in% c(misc.list) 
name.indep <- names(full_om[!location])
full_om <- full_om[name.indep]

```

Preparation before running the estimation: 

```{r results='hide', message=FALSE, warning=FALSE}
# model list
model.list <- c("Ridge regression", "Lasso Regression")
out.list <- c("Model", "AUC", "95%-CI", "N")

# Bootstrap runs
runs <- 2

#values of the tuning paramter
	
grid = 10 ^ seq(2, -3, length=100)

# confidence intervals
n.ci <- 3
ci <- c(0.99, 0.95, 0.9)

#objects to stroe estimation results
aucs <- matrix(nrow=2, ncol=runs)
ci95_lo <- matrix(nrow=2, ncol=runs)
ci95_up <- matrix(nrow=2, ncol=runs)
N <- matrix(nrow=2, ncol=runs)
out <- matrix(nrow=2, ncol=5)


```

Ridge regression:

```{r results='hide', message=FALSE, warning=FALSE}
#drop alternative independent variables
for(j in 1:runs) {
	
	# training, test sample
	set.seed(j)
	indexes = sample(1:nrow(full_om), size=0.632*nrow(full_om), replace=F)
	test = full_om[-indexes,]
	train = full_om[indexes,]
	
	# Ridge Regression
	train.mat = model.matrix(b2~., data=train) 
  test.mat = model.matrix(b2~., data=test)
  mod.ridge = cv.glmnet(train.mat, train[, "b2"], alpha=0, lambda=grid, family="binomial", thresh=1e-12)
  lambda.best = mod.ridge$lambda.min 
  
  # OOS-analysis
  ridge.prob = predict(mod.ridge,newx=test.mat, s=lambda.best, type="response") # predicted outcome

  #N[1,j] <- mod.ridge[["glmnet.fit"]][["nobs"]] #if we really reproduce Table 3, we should put just number of obs

	true<-test[,"b2"] # real outcome

	r<-roc(true,as.numeric(ridge.prob),ci=T) # ROC analysis
	
	aucs[1,j] <- as.numeric(r$auc)
		
	ci95_lo[1,j] <- as.numeric(ci.auc(r,conf.level=ci[2]))[1]
	ci95_up[1,j] <- as.numeric(ci.auc(r,conf.level=ci[2]))[3]

}

n_ridge <- trunc(as.numeric(colMeans(as.matrix(N[1, ])))) # update output table matrix

auc_ridge<-as.numeric(colMeans(as.matrix(aucs[1, ])))
ci95_lo_ridge<-as.numeric(colMeans(as.matrix(ci95_lo[1, ])))
ci95_up_ridge<-as.numeric(colMeans(as.matrix(ci95_up[1, ])))

out[1,1]<-model.list[1]
out[1,2]<-auc_ridge
out[1,3]<-ci95_lo_ridge
out[1,4]<-ci95_up_ridge
out[1,5]<-dim(full_om)[1]

```

Lasso Regression 

```{r results='hide', warning=FALSE, message=FALSE}
for(j in 1:runs) {
	
	# training, test sample
	set.seed(j+100)
	indexes = sample(1:nrow(full_om), size=0.632*nrow(full_om), replace=F)
	test = full_om[-indexes,]
	train = full_om[indexes,]
	
  #Lasso regression
	train.mat = model.matrix(b2~., data=train) 
  test.mat = model.matrix(b2~., data=test)
  mod.lasso = cv.glmnet(train.mat, train[, "b2"], alpha=1, lambda=grid, family="binomial", thresh=1e-12)
  lambda.best = mod.lasso$lambda.min 
  
  # OOS-analysis
  lasso.prob = predict(mod.lasso,newx=test.mat, s=lambda.best, type="response") # predicted outcome

  #N[2,j] <- mod.lasso[["glmnet.fit"]][["nobs"]]

	true<-test[,"b2"] # real outcome

	r<-roc(true,lasso.prob,ci=T) # ROC analysis
	
	aucs[2,j] <- as.numeric(r$auc)
		
	ci95_lo[2,j] <- as.numeric(ci.auc(r,conf.level=ci[2]))[1]
	ci95_up[2,j] <- as.numeric(ci.auc(r,conf.level=ci[2]))[3]

}

n_lasso <- trunc(as.numeric(colMeans(as.matrix(N[2, ])))) 
auc_lasso<-as.numeric(colMeans(as.matrix(aucs[2, ])))
ci95_lo_lasso<-as.numeric(colMeans(as.matrix(ci95_lo[2, ])))
ci95_up_lasso<-as.numeric(colMeans(as.matrix(ci95_up[2, ])))

out[2,1]<-model.list[2]
out[2,2]<-auc_lasso
out[2,3]<-ci95_lo_lasso
out[2,4]<-ci95_up_lasso
out[2,5]<-dim(full_om)[1]
```

Output table:

```{r}
#OUTPUT TABLE (always use double the amount of backslashes needed in latex)

# reformat decimals
out2<-out
out2[,2:5]<-round(as.numeric(out2[,2:5]), digits=2)

# confidence intervals
cis<-paste(out2[,3], out2[,4], sep=",")
cis<-paste("[", cis, sep="")
cis<-paste(cis, "]", sep="")

out3 <- out2[,c(1,2,5)] # leave out .9, .99 lower-ci columns
out4 <- cbind(out3,cis)
outF <- out4[,c(1,2,4,3)] 
colnames(outF)<-out.list
outF<-as.data.frame(outF)

kable(outF, "latex", booktabs = T,row.names = FALSE) %>%
  kable_styling(latex_options = "striped")

```

The first observation we can make is that both model performs very similarly. Ridge does slightly better in terms of the AUC, our reference metric of misclassification. This result is however striking. Indeed, the number of regressors which Lasso shrinks to zero is extremely high (44 out of 76). In such cases, where most of the regressors have no predicting power, lasso should perform relatively better theoretically.  
When it comes to compare our estimations techniques with those of Table III in the paper in the many predictors case, we notice that they stand in the middle between the Single tree and Bagging/Random forest results. With the regard to Single Tree, we know that its relatively bad performance is due to the high variance component in the MSE. This is proved by the fact that in Bagging and Random Forest AUC jumps up substantially. The performance of these latter two estimations techniques is considerably superior to the multivariate logit benchmark. Even if the set of predictors used in the logit and the the other "ensamble" methods is different, this suggests that the relation between the dependent and the independent variable is better approximated by a non-paramteric, more flexible technique as the tree-based ones rather than a specific functional form, as a logistic function. We could read our results in light of this observation. Even if Ridge and Lasso are much more "flexible" with respect to plain logit as they can select (or give more weight) to most important (or "true") predictors and at the same time avoid unstable estimates, they still rely on a logistic functional form, which proved to fit poorly the data under analysis.

##Question 2
# a) 

Plain lasso optimization problem
$$  \hat{\bm \beta_{L}} =\arg\min_{\bm \beta}\left[ \Vert\bm y-\sum_{j=1}^{p} \bm x_j \beta_j \Vert^2 + \lambda\sum_{j=1}^{p}|\beta_j|\right]$$
Adaptive lasso optimization problem
$$  \hat{\bm \beta_{AL}} =\arg\min_{\bm \beta}\left[ \Vert\bm y-\sum_{j=1}^{p} \bm x_j \beta_j \Vert^2 + \lambda\sum_{j=1}^{p}|\beta_j|w_j\right]$$
with $w_i=\frac{1}{|\hat\beta_P|^{\gamma}}$, $\gamma\in\{1,2\}, \bm\beta_P$ pre-estimator.
Plain lasso is a less parsimonious model, it is suggested when we are interested in out-of-sample performance of our model (prediction).  
On the contrary, adaptive lasso is recommended when we are interested in the in-sample properties of the estimated parameters. The weight $w_j$ is inversely proportional to the magnitude of the coefficients in the preestimation: higher coefficients get penalized less in adaptive lasso. Therefore, coefficients that are smaller in the pre-estimation are potentially shrunk to zero. Adaptive lasso is therefore more parsimonious than plain lasso. Coefficients that would be close to zero in plain lasso are probably biased and in adaptive lasso they get penalized further and shrunk to zero. Under mild conditions, this method can select the true underlying model.  
The tuning parameters are usually selected from the cross-validated MSE. In plain lasso, there is one tuning paramter, $\lambda$, while in the adaptive lasso there are two, $\gamma$ and $\lambda$, so the cross-validation will be two-dimensional. Even if it does not have sound statistical foundations, sometimes it is suggested to pick not the $\lambda$ that minimizes the cross-validated MSE, but the one  that is one standard error above it.


# b)
The most intuitive reason to apply other estimation techniques after having found the true model by adaptive lasso (under some regularity conditions) is that shrinkage estimators are inherently biased. The penalty in the objective function ($\lambda$ above) enters the estimator formula, producing a bias. This bias tends to be high especially in finite samples. Therefore, it is reasonably better to apply adaptive lasso to find the covariates which characterize the true model and then apply an unbiased estimation technique on this restricted set of covariates.  
Antoher possible reason we can think about could be that in some estimation techniques (as OLS) the higher the number of regressors, the higher the variance of the final estimator. Therefore, this would end up in higher standard errors and this could eventually inflate p-values. Thus, running estimation on a smaller set of covariates would yield a more precise estimators, with lower standard errors. However, the significance of final coefficients would potentially not be a problem, as we assume that adaptive lasso already selects just the covariates different from 0.

# Question 3

Useful links https://arxiv.org/pdf/1509.09169.pdf https://www.statlect.com/fundamentals-of-statistics/ridge-regression


## a

Proof of BIAS:
$$
BIAS=E[\hat{\beta}(\lambda)|X]-\beta $$
\begin{align*}
E[\hat{\beta}(\lambda)|X]&=E[W(\lambda)X^T y|X]=E[W(\lambda)X^T (X\beta+\epsilon)|X] \\
\\
&= W(\lambda)X^T X\beta+W(\lambda)X^T \underbrace{E(\epsilon|X)}_{=0} = W(\lambda)X^T X\beta \\
&=(X^TX+\lambda I_p)^{-1}(X^TX+\lambda I_p-\lambda I_p)\beta=\\
&=\left[I_p-\lambda(X^TX+\lambda I_p)^{-1}\right]\beta\\
&=\beta-\lambda(X^TX+\lambda I_p)^{-1}\beta\\
&=\beta-\lambda W(\lambda)\beta
\end{align*}
Therefore, the bias is equal to
\begin{equation*}
BIAS=E[\hat{\beta}(\lambda)|X]-\beta=\beta-\lambda W(\lambda)\beta-\beta=-\lambda W(\lambda)\beta
\end{equation*}



%Anche qui manca un passaggio per arrivare alla risposta 



Proof of variance:
\begin{gather*}
Var[\hat{\beta}(\lambda)|X]=Var[W(\lambda)X^T Y|X]=Var[W(\lambda)X^T (X\beta+\epsilon)|X] \\
\\
=Var[(X^TX+\lambda I_p)^{-1}X^T (X\beta+\epsilon)|X]
\end{gather*}

Noticing that the first term is constant given $X$, $\lambda$ and $\beta$ and recalling that $Var(a+X)=Var(X)$ with $a$ being a constant; we can simplify as follows: \\
\begin{gather*}
Var[(X^TX+\lambda I_p)^{-1}X^T (X\beta+\epsilon)|X]=Var[W(\lambda)X^T\epsilon|X]=W(\lambda)X^TVar[\epsilon|X]XW(\lambda)^T \\
=W(\lambda)X^T\sigma^2I_T X W(\lambda)^T=\sigma^2W(\lambda)(X^TX) W(\lambda)^T
\end{gather*}

We then can write:

\begin{gather*}
Var[\hat{\beta}(0)|X]-Var[\hat{\beta}(\lambda)|X]=\sigma^2[(X^TX)^{-1}-W(\lambda)(X^TX) W(\lambda)^T] \\
= \sigma^2 W(\lambda)[W(\lambda)^{-1}(X^TX)^{-1}(W(\lambda)^T)^{-1}-(X^TX)]W(\lambda)^T \\
= \sigma^2 W(\lambda)[[X^TX+\lambda I_p](X^TX)^{-1}[X^TX+\lambda I_p]^T-(X^TX)]W(\lambda)^T \\
\implies Var[\hat{\beta}(0)|X]-Var[\hat{\beta}(\lambda)|X]=\sigma^2W_{\lambda}[2\lambda I_p+\lambda^2(X^T X)^{-1}]W_{\lambda}
\end{gather*}
\newline
It is easy to see that for $\lambda>0$, this matrix is positive definite because for any $\nu \neq 0$, we have:

\[ z=W_\lambda \nu \neq 0\] 
and 

\begin{gather*}
\nu^T[Var[\hat{\beta}(0)|X]-Var[\hat{\beta}(\lambda)|X]]\nu=\sigma^2 z^T[2\lambda I_p+\lambda^2(X^TX)^{-1}]z \\
=\sigma^2 \lambda z^Tz+\sigma^2\lambda^2z^T(X^TX)^{-1}z>0
\end{gather*}

This shows that although biased, the ridge estimator is more efficient than the OLS one. 


We have that:

\begin{gather*}
PMSE(\beta(\lambda)|X)=E\left[|| X\hat{\beta}(\lambda)-X\beta||^2 \ | X\right]=E[(\hat{\beta}(\lambda)-\beta)^TX^TX(\hat{\beta}(\lambda)-\beta) \ | X] \\
\end{gather*}

Then taking the difference between $PSME(\beta(0)|X)-PSME(\beta(\lambda)|X)$ gives:

\begin{gather*}
E[(\hat{\beta}(0)-\beta)^TX^TX(\hat{\beta}(0)-\beta) \ | X]-E[(\hat{\beta}(\lambda)-\beta)^TX^TX(\hat{\beta}(\lambda)-\beta) \ | X] \\
=E[(\hat{\beta}(0) - \hat{\beta}(\lambda))'X'X(\hat{\beta}(\lambda)+\hat{\beta}(0)-2\beta)| X ]
\end{gather*}

%Ho appena visto che su Canvas in discussion ci sono dei suggerimenti, si dovrebbe riuscire con quelli 
Then I've tried different ways to simplify but couldn't get the answer 
\[ \lambda Tr{W_\lambda^TW_\lambda[2\sigma^2(X^TX)+\lambda(\sigma^2I_p-\beta\beta^TX^TX)]}  \]



In order to show that for a given X the ridge estimator dominates the OLS one for PMSE, we want to show that the above difference is indeed positive when the matrix $\sigma^2I_p-\beta\beta^TX^TX$ is positive definite. For the trace to be always positive, we need all the eigenvalues of the matrix to be positive. This is the case when the whole matrix is positive definite. Hence, in what follows we need to show that when $\sigma^2I_p-\beta\beta^TX^TX$ is positive definite, then consequently $W_\lambda^TW_\lambda[2\sigma^2(X^TX)+\lambda(\sigma^2I_p-\beta\beta^TX^TX)]$ is positive definite (having $\lambda >0$). 
\newline
We first recall some properties of definiteness of matrices: 
- If A and B are positive definite, then the sum $A + B$ is also positive definite.
- If A and B are positive definite, then the products $ABA$ and $BAB$ are also positive definite. If $AB = BA$, then $AB$ is also positive definite.

From the former we get that $M=2\sigma^2(X^TX)+\lambda(\sigma^2I_p-\beta\beta^TX^TX)$ is positive definite, since it is straightforward that the quadratic matrix multiplied by a positive constant will be positive definite. For the same reason we also know that $N=W_\lambda^TW_{\lambda}$ is positive definite. 

Now we need to show that $MN=NM$

Non so se sia meglio questo approccio o prendere un vettore $z \neq 0$ e far vedere che z'MNz è sempre >0 











\end{document}







