% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={Data Science Methods - Assignment 2},
  pdfauthor={M. Alberti, s.n. 2020162; N. Ceschin, s.n. 344510},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage[margin=1in]{geometry}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
\usepackage{bm}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}

\title{Data Science Methods - Assignment 2}
\author{M. Alberti, s.n. 2020162 \and N. Ceschin, s.n. 344510}
\date{March 16, 2020}

\begin{document}
\maketitle

\hypertarget{question-1}{%
\section{Question 1}\label{question-1}}

First we upload all relevant libraries:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{rm}\NormalTok{(}\DataTypeTok{list=}\KeywordTok{ls}\NormalTok{())}

\CommentTok{# load libraries}
\KeywordTok{library}\NormalTok{(leaps)}
\KeywordTok{library}\NormalTok{(glmnet) }\CommentTok{# for lasso and ridge}
\KeywordTok{library}\NormalTok{(Matrix)}
\KeywordTok{library}\NormalTok{(pROC)}
\KeywordTok{library}\NormalTok{(xtable)}
\KeywordTok{library}\NormalTok{(knitr)}
\KeywordTok{library}\NormalTok{(kableExtra)}
\KeywordTok{library}\NormalTok{(dplyr)}
\end{Highlighting}
\end{Shaded}

Data preparation

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#setwd("C:/Users/nicol/Documents/Tilburg/Courses/Data Science Methods/DATA-SCIENCE-ASSIGNMENTS/Assignment 2")}
\KeywordTok{setwd}\NormalTok{(}\StringTok{"C:/Users/Mr Nobody/Desktop/Uni/EME/Data science Methods/Assignments/Assignment 2"}\NormalTok{)}

\NormalTok{data<-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\StringTok{"R_class.csv"}\NormalTok{, }\DataTypeTok{sep=}\StringTok{","}\NormalTok{, }\DataTypeTok{dec=}\StringTok{"."}\NormalTok{, }\DataTypeTok{header=}\OtherTok{TRUE}\NormalTok{)}

\NormalTok{ca <-}\StringTok{ }\KeywordTok{grep}\NormalTok{(}\StringTok{"ca"}\NormalTok{, }\KeywordTok{names}\NormalTok{(data), }\DataTypeTok{value=}\NormalTok{T)  }\CommentTok{#vector of names of columns containing ca}
\NormalTok{drops <-}\StringTok{ }\KeywordTok{names}\NormalTok{(data) }\OperatorTok{%in%}\StringTok{ }\KeywordTok{c}\NormalTok{(ca)}
\NormalTok{data <-}\StringTok{ }\NormalTok{data[}\OperatorTok{!}\NormalTok{drops]  }

\CommentTok{# drop vars not used}
\NormalTok{assets <-}\StringTok{ }\KeywordTok{grep}\NormalTok{(}\StringTok{"assets"}\NormalTok{, }\KeywordTok{names}\NormalTok{(data), }\DataTypeTok{value=}\NormalTok{T)}
\NormalTok{stocks <-}\StringTok{ }\KeywordTok{grep}\NormalTok{(}\StringTok{"stocks"}\NormalTok{, }\KeywordTok{names}\NormalTok{(data), }\DataTypeTok{value=}\NormalTok{T)}
\NormalTok{narrowm <-}\StringTok{ }\KeywordTok{grep}\NormalTok{(}\StringTok{"narrowm"}\NormalTok{, }\KeywordTok{names}\NormalTok{(data), }\DataTypeTok{value=}\NormalTok{T)}
\NormalTok{money <-}\StringTok{ }\KeywordTok{grep}\NormalTok{(}\StringTok{"money"}\NormalTok{, }\KeywordTok{names}\NormalTok{(data), }\DataTypeTok{value=}\NormalTok{T)}
\NormalTok{ltrate <-}\StringTok{ }\KeywordTok{grep}\NormalTok{(}\StringTok{"ltrate"}\NormalTok{, }\KeywordTok{names}\NormalTok{(data), }\DataTypeTok{value=}\NormalTok{T)}
\NormalTok{stir <-}\StringTok{ }\KeywordTok{grep}\NormalTok{(}\StringTok{"stir"}\NormalTok{, }\KeywordTok{names}\NormalTok{(data), }\DataTypeTok{value=}\NormalTok{T)}
\NormalTok{loans <-}\StringTok{ }\KeywordTok{grep}\NormalTok{(}\StringTok{"loans"}\NormalTok{, }\KeywordTok{names}\NormalTok{(data), }\DataTypeTok{value=}\NormalTok{T)}
\NormalTok{debt <-}\StringTok{ }\KeywordTok{grep}\NormalTok{(}\StringTok{"debt"}\NormalTok{, }\KeywordTok{names}\NormalTok{(data), }\DataTypeTok{value=}\NormalTok{T)}
\NormalTok{er <-}\StringTok{ }\KeywordTok{grep}\NormalTok{(}\StringTok{"er"}\NormalTok{, }\KeywordTok{names}\NormalTok{(data), }\DataTypeTok{value=}\NormalTok{T)}
\NormalTok{cpi <-}\StringTok{ }\KeywordTok{grep}\NormalTok{(}\StringTok{"cpi"}\NormalTok{, }\KeywordTok{names}\NormalTok{(data), }\DataTypeTok{value=}\NormalTok{T)}
\NormalTok{gap <-}\StringTok{ }\KeywordTok{grep}\NormalTok{(}\StringTok{"gap"}\NormalTok{, }\KeywordTok{names}\NormalTok{(data), }\DataTypeTok{value=}\NormalTok{T)}
\NormalTok{glo <-}\StringTok{ }\KeywordTok{grep}\NormalTok{(}\StringTok{"a_"}\NormalTok{, }\KeywordTok{names}\NormalTok{(data), }\DataTypeTok{value=}\NormalTok{T)}
\NormalTok{gdp <-}\StringTok{ }\KeywordTok{grep}\NormalTok{(}\StringTok{"gdp"}\NormalTok{, }\KeywordTok{names}\NormalTok{(data), }\DataTypeTok{value=}\NormalTok{T)}
\NormalTok{i <-}\StringTok{ }\KeywordTok{grep}\NormalTok{(}\StringTok{"i_"}\NormalTok{, }\KeywordTok{names}\NormalTok{(data), }\DataTypeTok{value=}\NormalTok{T)}
\NormalTok{c <-}\StringTok{ }\KeywordTok{grep}\NormalTok{(}\StringTok{"c_"}\NormalTok{, }\KeywordTok{names}\NormalTok{(data), }\DataTypeTok{value=}\NormalTok{T)}
\NormalTok{ri <-}\StringTok{ }\KeywordTok{grep}\NormalTok{(}\StringTok{"ri"}\NormalTok{, }\KeywordTok{names}\NormalTok{(data), }\DataTypeTok{value=}\NormalTok{T)}
\NormalTok{rc <-}\StringTok{ }\KeywordTok{grep}\NormalTok{(}\StringTok{"rc"}\NormalTok{, }\KeywordTok{names}\NormalTok{(data), }\DataTypeTok{value=}\NormalTok{T)}

\NormalTok{drops <-}\StringTok{ }\KeywordTok{names}\NormalTok{(data) }\OperatorTok{%in%}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"year"}\NormalTok{, }\StringTok{"ccode"}\NormalTok{, stocks, money, stir,assets,i,ri,glo) }
\NormalTok{saves <-}\StringTok{ }\KeywordTok{names}\NormalTok{(data) }\OperatorTok{%in%}\StringTok{ }\KeywordTok{c}\NormalTok{(glo)}
\NormalTok{full <-}\StringTok{ }\NormalTok{data[}\OperatorTok{!}\NormalTok{drops] }\CommentTok{# drops those variables which have true indication in "drops"}
\NormalTok{full <-}\StringTok{ }\KeywordTok{cbind}\NormalTok{(data[glo], full)}

\CommentTok{# FULL SET: omit observations with missing values}
\NormalTok{full_om <-}\StringTok{ }\KeywordTok{na.omit}\NormalTok{(full)}
\NormalTok{misc.list <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"b1"}\NormalTok{,}\StringTok{"b3"}\NormalTok{,}\StringTok{"rec1"}\NormalTok{,}\StringTok{"rec2"}\NormalTok{,}\StringTok{"rec3"}\NormalTok{)}
\NormalTok{location <-}\StringTok{ }\KeywordTok{names}\NormalTok{(full_om) }\OperatorTok{%in%}\StringTok{ }\KeywordTok{c}\NormalTok{(misc.list) }
\NormalTok{name.indep <-}\StringTok{ }\KeywordTok{names}\NormalTok{(full_om[}\OperatorTok{!}\NormalTok{location])}
\NormalTok{full_om <-}\StringTok{ }\NormalTok{full_om[name.indep]}
\end{Highlighting}
\end{Shaded}

Preparation before running the estimation:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# model list}
\NormalTok{model.list <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Ridge regression"}\NormalTok{, }\StringTok{"Lasso Regression"}\NormalTok{)}
\NormalTok{out.list <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Model"}\NormalTok{, }\StringTok{"AUC"}\NormalTok{, }\StringTok{"95%-CI"}\NormalTok{, }\StringTok{"N"}\NormalTok{)}

\CommentTok{# Bootstrap runs}
\NormalTok{runs <-}\StringTok{ }\DecValTok{2}

\CommentTok{#values of the tuning paramter}
    
\NormalTok{grid =}\StringTok{ }\DecValTok{10} \OperatorTok{^}\StringTok{ }\KeywordTok{seq}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{-3}\NormalTok{, }\DataTypeTok{length=}\DecValTok{100}\NormalTok{)}

\CommentTok{# confidence intervals}
\NormalTok{n.ci <-}\StringTok{ }\DecValTok{3}
\NormalTok{ci <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\FloatTok{0.99}\NormalTok{, }\FloatTok{0.95}\NormalTok{, }\FloatTok{0.9}\NormalTok{)}

\CommentTok{#objects to stroe estimation results}
\NormalTok{aucs <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(}\DataTypeTok{nrow=}\DecValTok{2}\NormalTok{, }\DataTypeTok{ncol=}\NormalTok{runs)}
\NormalTok{ci95_lo <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(}\DataTypeTok{nrow=}\DecValTok{2}\NormalTok{, }\DataTypeTok{ncol=}\NormalTok{runs)}
\NormalTok{ci95_up <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(}\DataTypeTok{nrow=}\DecValTok{2}\NormalTok{, }\DataTypeTok{ncol=}\NormalTok{runs)}
\NormalTok{N <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(}\DataTypeTok{nrow=}\DecValTok{2}\NormalTok{, }\DataTypeTok{ncol=}\NormalTok{runs)}
\NormalTok{out <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(}\DataTypeTok{nrow=}\DecValTok{2}\NormalTok{, }\DataTypeTok{ncol=}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Ridge regression:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#drop alternative independent variables}
\ControlFlowTok{for}\NormalTok{(j }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\NormalTok{runs) \{}
    
    \CommentTok{# training, test sample}
    \KeywordTok{set.seed}\NormalTok{(j)}
\NormalTok{    indexes =}\StringTok{ }\KeywordTok{sample}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\KeywordTok{nrow}\NormalTok{(full_om), }\DataTypeTok{size=}\FloatTok{0.632}\OperatorTok{*}\KeywordTok{nrow}\NormalTok{(full_om), }\DataTypeTok{replace=}\NormalTok{F)}
\NormalTok{    test =}\StringTok{ }\NormalTok{full_om[}\OperatorTok{-}\NormalTok{indexes,]}
\NormalTok{    train =}\StringTok{ }\NormalTok{full_om[indexes,]}
    
    \CommentTok{# Ridge Regression}
\NormalTok{    train.mat =}\StringTok{ }\KeywordTok{model.matrix}\NormalTok{(b2}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data=}\NormalTok{train) }
\NormalTok{  test.mat =}\StringTok{ }\KeywordTok{model.matrix}\NormalTok{(b2}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data=}\NormalTok{test)}
\NormalTok{  mod.ridge =}\StringTok{ }\KeywordTok{cv.glmnet}\NormalTok{(train.mat, train[, }\StringTok{"b2"}\NormalTok{], }\DataTypeTok{alpha=}\DecValTok{0}\NormalTok{, }\DataTypeTok{lambda=}\NormalTok{grid, }\DataTypeTok{family=}\StringTok{"binomial"}\NormalTok{, }\DataTypeTok{thresh=}\FloatTok{1e-12}\NormalTok{)}
\NormalTok{  lambda.best =}\StringTok{ }\NormalTok{mod.ridge}\OperatorTok{$}\NormalTok{lambda.min }
  
  \CommentTok{# OOS-analysis}
\NormalTok{  ridge.prob =}\StringTok{ }\KeywordTok{predict}\NormalTok{(mod.ridge,}\DataTypeTok{newx=}\NormalTok{test.mat, }\DataTypeTok{s=}\NormalTok{lambda.best, }\DataTypeTok{type=}\StringTok{"response"}\NormalTok{) }\CommentTok{# predicted outcome}

  \CommentTok{#N[1,j] <- mod.ridge[["glmnet.fit"]][["nobs"]] #if we really reproduce Table 3, we should put just number of obs}

\NormalTok{    true<-test[,}\StringTok{"b2"}\NormalTok{] }\CommentTok{# real outcome}

\NormalTok{    r<-}\KeywordTok{roc}\NormalTok{(true,}\KeywordTok{as.numeric}\NormalTok{(ridge.prob),}\DataTypeTok{ci=}\NormalTok{T) }\CommentTok{# ROC analysis}
    
\NormalTok{    aucs[}\DecValTok{1}\NormalTok{,j] <-}\StringTok{ }\KeywordTok{as.numeric}\NormalTok{(r}\OperatorTok{$}\NormalTok{auc)}
        
\NormalTok{    ci95_lo[}\DecValTok{1}\NormalTok{,j] <-}\StringTok{ }\KeywordTok{as.numeric}\NormalTok{(}\KeywordTok{ci.auc}\NormalTok{(r,}\DataTypeTok{conf.level=}\NormalTok{ci[}\DecValTok{2}\NormalTok{]))[}\DecValTok{1}\NormalTok{]}
\NormalTok{    ci95_up[}\DecValTok{1}\NormalTok{,j] <-}\StringTok{ }\KeywordTok{as.numeric}\NormalTok{(}\KeywordTok{ci.auc}\NormalTok{(r,}\DataTypeTok{conf.level=}\NormalTok{ci[}\DecValTok{2}\NormalTok{]))[}\DecValTok{3}\NormalTok{]}

\NormalTok{\}}

\NormalTok{n_ridge <-}\StringTok{ }\KeywordTok{trunc}\NormalTok{(}\KeywordTok{as.numeric}\NormalTok{(}\KeywordTok{colMeans}\NormalTok{(}\KeywordTok{as.matrix}\NormalTok{(N[}\DecValTok{1}\NormalTok{, ])))) }\CommentTok{# update output table matrix}

\NormalTok{auc_ridge<-}\KeywordTok{as.numeric}\NormalTok{(}\KeywordTok{colMeans}\NormalTok{(}\KeywordTok{as.matrix}\NormalTok{(aucs[}\DecValTok{1}\NormalTok{, ])))}
\NormalTok{ci95_lo_ridge<-}\KeywordTok{as.numeric}\NormalTok{(}\KeywordTok{colMeans}\NormalTok{(}\KeywordTok{as.matrix}\NormalTok{(ci95_lo[}\DecValTok{1}\NormalTok{, ])))}
\NormalTok{ci95_up_ridge<-}\KeywordTok{as.numeric}\NormalTok{(}\KeywordTok{colMeans}\NormalTok{(}\KeywordTok{as.matrix}\NormalTok{(ci95_up[}\DecValTok{1}\NormalTok{, ])))}

\NormalTok{out[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{]<-model.list[}\DecValTok{1}\NormalTok{]}
\NormalTok{out[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{]<-auc_ridge}
\NormalTok{out[}\DecValTok{1}\NormalTok{,}\DecValTok{3}\NormalTok{]<-ci95_lo_ridge}
\NormalTok{out[}\DecValTok{1}\NormalTok{,}\DecValTok{4}\NormalTok{]<-ci95_up_ridge}
\NormalTok{out[}\DecValTok{1}\NormalTok{,}\DecValTok{5}\NormalTok{]<-}\KeywordTok{dim}\NormalTok{(full_om)[}\DecValTok{1}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

Lasso Regression

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{for}\NormalTok{(j }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\NormalTok{runs) \{}
    
    \CommentTok{# training, test sample}
    \KeywordTok{set.seed}\NormalTok{(j}\OperatorTok{+}\DecValTok{100}\NormalTok{)}
\NormalTok{    indexes =}\StringTok{ }\KeywordTok{sample}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\KeywordTok{nrow}\NormalTok{(full_om), }\DataTypeTok{size=}\FloatTok{0.632}\OperatorTok{*}\KeywordTok{nrow}\NormalTok{(full_om), }\DataTypeTok{replace=}\NormalTok{F)}
\NormalTok{    test =}\StringTok{ }\NormalTok{full_om[}\OperatorTok{-}\NormalTok{indexes,]}
\NormalTok{    train =}\StringTok{ }\NormalTok{full_om[indexes,]}
    
  \CommentTok{#Lasso regression}
\NormalTok{    train.mat =}\StringTok{ }\KeywordTok{model.matrix}\NormalTok{(b2}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data=}\NormalTok{train) }
\NormalTok{  test.mat =}\StringTok{ }\KeywordTok{model.matrix}\NormalTok{(b2}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data=}\NormalTok{test)}
\NormalTok{  mod.lasso =}\StringTok{ }\KeywordTok{cv.glmnet}\NormalTok{(train.mat, train[, }\StringTok{"b2"}\NormalTok{], }\DataTypeTok{alpha=}\DecValTok{1}\NormalTok{, }\DataTypeTok{lambda=}\NormalTok{grid, }\DataTypeTok{family=}\StringTok{"binomial"}\NormalTok{, }\DataTypeTok{thresh=}\FloatTok{1e-12}\NormalTok{)}
\NormalTok{  lambda.best =}\StringTok{ }\NormalTok{mod.lasso}\OperatorTok{$}\NormalTok{lambda.min }
  
  \CommentTok{# OOS-analysis}
\NormalTok{  lasso.prob =}\StringTok{ }\KeywordTok{predict}\NormalTok{(mod.lasso,}\DataTypeTok{newx=}\NormalTok{test.mat, }\DataTypeTok{s=}\NormalTok{lambda.best, }\DataTypeTok{type=}\StringTok{"response"}\NormalTok{) }\CommentTok{# predicted outcome}

  \CommentTok{#N[2,j] <- mod.lasso[["glmnet.fit"]][["nobs"]]}

\NormalTok{    true<-test[,}\StringTok{"b2"}\NormalTok{] }\CommentTok{# real outcome}

\NormalTok{    r<-}\KeywordTok{roc}\NormalTok{(true,lasso.prob,}\DataTypeTok{ci=}\NormalTok{T) }\CommentTok{# ROC analysis}
    
\NormalTok{    aucs[}\DecValTok{2}\NormalTok{,j] <-}\StringTok{ }\KeywordTok{as.numeric}\NormalTok{(r}\OperatorTok{$}\NormalTok{auc)}
        
\NormalTok{    ci95_lo[}\DecValTok{2}\NormalTok{,j] <-}\StringTok{ }\KeywordTok{as.numeric}\NormalTok{(}\KeywordTok{ci.auc}\NormalTok{(r,}\DataTypeTok{conf.level=}\NormalTok{ci[}\DecValTok{2}\NormalTok{]))[}\DecValTok{1}\NormalTok{]}
\NormalTok{    ci95_up[}\DecValTok{2}\NormalTok{,j] <-}\StringTok{ }\KeywordTok{as.numeric}\NormalTok{(}\KeywordTok{ci.auc}\NormalTok{(r,}\DataTypeTok{conf.level=}\NormalTok{ci[}\DecValTok{2}\NormalTok{]))[}\DecValTok{3}\NormalTok{]}

\NormalTok{\}}

\NormalTok{n_lasso <-}\StringTok{ }\KeywordTok{trunc}\NormalTok{(}\KeywordTok{as.numeric}\NormalTok{(}\KeywordTok{colMeans}\NormalTok{(}\KeywordTok{as.matrix}\NormalTok{(N[}\DecValTok{2}\NormalTok{, ])))) }
\NormalTok{auc_lasso<-}\KeywordTok{as.numeric}\NormalTok{(}\KeywordTok{colMeans}\NormalTok{(}\KeywordTok{as.matrix}\NormalTok{(aucs[}\DecValTok{2}\NormalTok{, ])))}
\NormalTok{ci95_lo_lasso<-}\KeywordTok{as.numeric}\NormalTok{(}\KeywordTok{colMeans}\NormalTok{(}\KeywordTok{as.matrix}\NormalTok{(ci95_lo[}\DecValTok{2}\NormalTok{, ])))}
\NormalTok{ci95_up_lasso<-}\KeywordTok{as.numeric}\NormalTok{(}\KeywordTok{colMeans}\NormalTok{(}\KeywordTok{as.matrix}\NormalTok{(ci95_up[}\DecValTok{2}\NormalTok{, ])))}

\NormalTok{out[}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{]<-model.list[}\DecValTok{2}\NormalTok{]}
\NormalTok{out[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{]<-auc_lasso}
\NormalTok{out[}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{]<-ci95_lo_lasso}
\NormalTok{out[}\DecValTok{2}\NormalTok{,}\DecValTok{4}\NormalTok{]<-ci95_up_lasso}
\NormalTok{out[}\DecValTok{2}\NormalTok{,}\DecValTok{5}\NormalTok{]<-}\KeywordTok{dim}\NormalTok{(full_om)[}\DecValTok{1}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

Output table:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#OUTPUT TABLE (always use double the amount of backslashes needed in latex)}

\CommentTok{# reformat decimals}
\NormalTok{out2<-out}
\NormalTok{out2[,}\DecValTok{2}\OperatorTok{:}\DecValTok{5}\NormalTok{]<-}\KeywordTok{round}\NormalTok{(}\KeywordTok{as.numeric}\NormalTok{(out2[,}\DecValTok{2}\OperatorTok{:}\DecValTok{5}\NormalTok{]), }\DataTypeTok{digits=}\DecValTok{2}\NormalTok{)}

\CommentTok{# confidence intervals}
\NormalTok{cis<-}\KeywordTok{paste}\NormalTok{(out2[,}\DecValTok{3}\NormalTok{], out2[,}\DecValTok{4}\NormalTok{], }\DataTypeTok{sep=}\StringTok{","}\NormalTok{)}
\NormalTok{cis<-}\KeywordTok{paste}\NormalTok{(}\StringTok{"["}\NormalTok{, cis, }\DataTypeTok{sep=}\StringTok{""}\NormalTok{)}
\NormalTok{cis<-}\KeywordTok{paste}\NormalTok{(cis, }\StringTok{"]"}\NormalTok{, }\DataTypeTok{sep=}\StringTok{""}\NormalTok{)}

\NormalTok{out3 <-}\StringTok{ }\NormalTok{out2[,}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{5}\NormalTok{)] }\CommentTok{# leave out .9, .99 lower-ci columns}
\NormalTok{out4 <-}\StringTok{ }\KeywordTok{cbind}\NormalTok{(out3,cis)}
\NormalTok{outF <-}\StringTok{ }\NormalTok{out4[,}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{3}\NormalTok{)] }
\KeywordTok{colnames}\NormalTok{(outF)<-out.list}
\NormalTok{outF<-}\KeywordTok{as.data.frame}\NormalTok{(outF)}

\KeywordTok{kable}\NormalTok{(outF, }\StringTok{"latex"}\NormalTok{, }\DataTypeTok{booktabs =}\NormalTok{ T,}\DataTypeTok{row.names =} \OtherTok{FALSE}\NormalTok{) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{kable_styling}\NormalTok{(}\DataTypeTok{latex_options =} \StringTok{"striped"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}[H]
\centering
\begin{tabular}{llll}
\toprule
Model & AUC & 95\%-CI & N\\
\midrule
\rowcolor{gray!6}  Ridge regression & 0.78 & [0.72,0.85] & 1742\\
Lasso Regression & 0.76 & [0.69,0.84] & 1742\\
\bottomrule
\end{tabular}
\end{table}

The first observation we can make is that both model performs very
similarly. Ridge does slightly better in terms of the AUC, our reference
metric of misclassification. This result is however striking. Indeed,
the number of regressors which Lasso shrinks to zero is extremely high
(44 out of 76). In such cases, where most of the regressors have no
predicting power, lasso should perform relatively better
theoretically.\\
When it comes to compare our estimations techniques with those of Table
III in the paper in the many predictors case, we notice that they stand
in the middle between the Single tree and Bagging/Random forest results.
With the regard to Single Tree, we know that its relatively bad
performance is due to the high variance component in the MSE. This is
proved by the fact that in Bagging and Random Forest AUC jumps up
substantially. The performance of these latter two estimations
techniques is considerably superior to the multivariate logit benchmark.
Even if the set of predictors used in the logit and the the other
``ensamble'' methods is different, this suggests that the relation
between the dependent and the independent variable is better
approximated by a non-paramteric, more flexible technique as the
tree-based ones rather than a specific functional form, as a logistic
function. We could read our results in light of this observation. Even
if Ridge and Lasso are much more ``flexible'' with respect to plain
logit as they can select (or give more weight) to most important (or
``true'') predictors and at the same time avoid unstable estimates, they
still rely on a logistic functional form, which proved to fit poorly the
data under analysis.

\hypertarget{question-2}{%
\section{Question 2}\label{question-2}}

\hypertarget{a}{%
\subsection{a)}\label{a}}

Plain lasso optimization problem
\[  \hat{\bm \beta_{L}} =\arg\min_{\bm \beta}\left[ \Vert\bm y-\sum_{j=1}^{p} \bm x_j \beta_j \Vert^2 + \lambda\sum_{j=1}^{p}|\beta_j|\right]\]
Adaptive lasso optimization problem
\[  \hat{\bm \beta_{AL}} =\arg\min_{\bm \beta}\left[ \Vert\bm y-\sum_{j=1}^{p} \bm x_j \beta_j \Vert^2 + \lambda\sum_{j=1}^{p}|\beta_j|w_j\right]\]
with \(w_i=\frac{1}{|\hat\beta_P|^{\gamma}}\),
\(\gamma\in\{1,2\}, \bm\beta_P\) pre-estimator. Plain lasso is a less
parsimonious model, it is suggested when we are interested in
out-of-sample performance of our model (prediction).\\
On the contrary, adaptive lasso is recommended when we are interested in
the in-sample properties of the estimated parameters. The weight \(w_j\)
is inversely proportional to the magnitude of the coefficients in the
preestimation: higher coefficients get penalized less in adaptive lasso.
Therefore, coefficients that are smaller in the pre-estimation are
potentially shrunk to zero. Adaptive lasso is therefore more
parsimonious than plain lasso. Coefficients that would be close to zero
in plain lasso are probably biased and in adaptive lasso they get
penalized further and shrunk to zero. Under mild conditions, this method
can select the true underlying model.\\
The tuning parameters are usually selected from the cross-validated MSE.
In plain lasso, there is one tuning paramter, \(\lambda\), while in the
adaptive lasso there are two, \(\gamma\) and \(\lambda\), so the
cross-validation will be two-dimensional. Even if it does not have sound
statistical foundations, sometimes it is suggested to pick not the
\(\lambda\) that minimizes the cross-validated MSE, but the one that is
one standard error above it.

\hypertarget{b}{%
\subsection{b)}\label{b}}

The most intuitive reason to apply other estimation techniques after
having found the true model by adaptive lasso (under some regularity
conditions) is that shrinkage estimators are inherently biased. The
penalty in the objective function (\(\lambda\) above) enters the
estimator formula, producing a bias. This bias tends to be high
especially in finite samples. Therefore, it is reasonably better to
apply adaptive lasso to find the covariates which characterize the true
model and then apply an unbiased estimation technique on this restricted
set of covariates.\\
Antoher possible reason we can think about could be that in some
estimation techniques (as OLS) the higher the number of regressors, the
higher the variance of the final estimator. Therefore, this would end up
in higher standard errors and this could eventually inflate p-values.
Thus, running estimation on a smaller set of covariates would yield a
more precise estimators, with lower standard errors. However, the
significance of final coefficients would potentially not be a problem,
as we assume that adaptive lasso already selects just the covariates
different from 0.

\hypertarget{question-3}{%
\section{Question 3}\label{question-3}}

\hypertarget{a-1}{%
\subsection{a)}\label{a-1}}

Proof of BIAS:

\[BIAS=E[\hat{\beta}(\lambda)|X]-\beta \] \begin{align*}
E[\hat{\beta}(\lambda)|X]&=E[W(\lambda)X^T y|X]=E[W(\lambda)X^T (X\beta+\epsilon)|X] \\
\\
&= W(\lambda)X^T X\beta+W(\lambda)X^T \underbrace{E(\epsilon|X)}_{=0} = W(\lambda)X^T X\beta \\
&=(X^TX+\lambda I_p)^{-1}(X^TX+\lambda I_p-\lambda I_p)\beta=\\
&=\left[I_p-\lambda(X^TX+\lambda I_p)^{-1}\right]\beta\\
&=\beta-\lambda(X^TX+\lambda I_p)^{-1}\beta\\
&=\beta-\lambda W(\lambda)\beta
\end{align*} Therefore, the bias is equal to:

\begin{equation*}
BIAS=E[\hat{\beta}(\lambda)|X]-\beta=\beta-\lambda W(\lambda)\beta-\beta=-\lambda W(\lambda)\beta
\end{equation*}

\hypertarget{b-1}{%
\subsection{b)}\label{b-1}}

Proof of variance: \begin{gather*}
Var[\hat{\beta}(\lambda)|X]=Var[W(\lambda)X^T Y|X]=Var[W(\lambda)X^T (X\beta+\epsilon)|X] \\
\\
=Var[(X^TX+\lambda I_p)^{-1}X^T (X\beta+\epsilon)|X]
\end{gather*}

Noticing that the first term is constant given \(X\), \(\lambda\) and
\(\beta\) and recalling that \(Var(a+X)=Var(X)\) with \(a\) being a
constant; we can simplify as follows:

\begin{gather*}
Var[(X^TX+\lambda I_p)^{-1}X^T (X\beta+\epsilon)|X]=Var[W(\lambda)X^T\epsilon|X]=W(\lambda)X^TVar[\epsilon|X]XW(\lambda)^T \\
=W(\lambda)X^T\sigma^2I_T X W(\lambda)^T=\sigma^2W(\lambda)(X^TX) W(\lambda)^T
\end{gather*}

We then can write:

\begin{gather*}
Var[\hat{\beta}(0)|X]-Var[\hat{\beta}(\lambda)|X]=\sigma^2[(X^TX)^{-1}-W(\lambda)(X^TX) W(\lambda)^T] \\
= \sigma^2 W(\lambda)[W(\lambda)^{-1}(X^TX)^{-1}(W(\lambda)^T)^{-1}-(X^TX)]W(\lambda)^T \\
= \sigma^2 W(\lambda)[[X^TX+\lambda I_p](X^TX)^{-1}[X^TX+\lambda I_p]^T-(X^TX)]W(\lambda)^T \\
\implies Var[\hat{\beta}(0)|X]-Var[\hat{\beta}(\lambda)|X]=\sigma^2W_{\lambda}[2\lambda I_p+\lambda^2(X^T X)^{-1}]W_{\lambda}
\end{gather*}

It is easy to see that for \(\lambda>0\), this matrix is positive
definite because for any \(\nu \neq 0\), we have:

\[ z=W_\lambda \nu \neq 0\]

and

\begin{gather*}
\nu^T[Var[\hat{\beta}(0)|X]-Var[\hat{\beta}(\lambda)|X]]\nu=\sigma^2 z^T[2\lambda I_p+\lambda^2(X^TX)^{-1}]z \\
=\sigma^2 \lambda z^Tz+\sigma^2\lambda^2z^T(X^TX)^{-1}z>0
\end{gather*}

This shows that although biased, the ridge estimator is more efficient
than the OLS one.

We have that:

\begin{gather*}
PMSE(\beta(\lambda)|X)=E\left[|| X\hat{\beta}(\lambda)-X\beta||^2 \ | X\right]=E[(\hat{\beta}(\lambda)-\beta)^TX^TX(\hat{\beta}(\lambda)-\beta) \ | X]
\end{gather*}

Then taking the difference between
\(PSME(\beta(0)|X)-PSME(\beta(\lambda)|X)\) gives:

\begin{gather*}
E[(\hat{\beta}(0)-\beta)^TX^TX(\hat{\beta}(0)-\beta) \ | X]-E[(\hat{\beta}(\lambda)-\beta)^TX^TX(\hat{\beta}(\lambda)-\beta) \ | X] \\
=E[(\hat{\beta}(0) - \hat{\beta}(\lambda))'X'X(\hat{\beta}(\lambda)+\hat{\beta}(0)-2\beta)| X ]
\end{gather*}

Then I've tried different ways to simplify but couldn't get the answer

\$\lambda Tr\{W\_\lambda\^{}TW\_\lambda[2\sigma^2(X^TX)+\lambda(\sigma^2I_p-\beta\beta^TX^TX)]\}
\$

In order to show that for a given X the ridge estimator dominates the
OLS one for PMSE, we want to show that the above difference is indeed
positive when the matrix \(\sigma^2I_p-\beta\beta^TX^TX\) is positive
definite. For the trace to be always positive, we need all the
eigenvalues of the matrix to be positive. This is the case when the
whole matrix is positive definite. Hence, in what follows we need to
show that when \(\sigma^2I_p-\beta\beta^TX^TX\) is positive definite,
then consequently
\(W_\lambda^TW_\lambda[2\sigma^2(X^TX)+\lambda(\sigma^2I_p-\beta\beta^TX^TX)]\)
is positive definite (having \(\lambda >0\)).

We first recall some properties of definiteness of matrices: - If A and
B are positive definite, then the sum \(A + B\) is also positive
definite. - If A and B are positive definite, then the products \(ABA\)
and \(BAB\) are also positive definite. If \(AB = BA\), then \(AB\) is
also positive definite.

From the former we get that
\(M=2\sigma^2(X^TX)+\lambda(\sigma^2I_p-\beta\beta^TX^TX)\) is positive
definite, since it is straightforward that the quadratic matrix
multiplied by a positive constant will be positive definite. For the
same reason we also know that \(N=W_\lambda^TW_{\lambda}\) is positive
definite.

Now we just need to show that \(W_\lambda^2 M=W_\lambda M W_{\lambda}\)

Let us start by noticing that \(W_{\lambda}=W_{\lambda}^T\), since the
transpose of a sum is the sum of transposes and \((X^TX)^T=(X^TX)\),
\$\lambda I\_p\textsuperscript{T=\lambda I\_p}T \$ since \(I_p^T=I_p\)
and lambda is a scalar.

\(A^{-1} B^{-1} =(BA)^{-1}\) , where \(A^{-1} = W(lambda)\), and
\(B^{-1}= M\). From this it follows that:

\begin{gather*}

    W_\lambda[2\sigma^2(X^TX)+\lambda(\sigma^2I_p-\beta\beta^TX^TX)]= A^{-1} B^{-1}
    =(BA)^{-1} \\
    \implies  W_\lambda[2\sigma^2(X^TX)+\lambda(\sigma^2I_p-\beta\beta^TX^TX)]=[2\sigma^2(X^TX)+\lambda(\sigma^2I_p-\beta\beta^TX^TX)]W_\lambda
    
\end{gather*}

Then we can finally write \$ W\_\lambda\^{}2 M=W\_\lambda M
W\_\lambda\$, where W and M are positive definite. From the second
property stated above, this product will be positive definite. Hence the
trace of this matrix, i.e.~the difference in PSME, will be greater than
zero, meaning that the ridge estimator dominates the OLS one in the
predictive mean square error sense.

\hypertarget{e}{%
\subsection{e)}\label{e}}

\textbackslash end\{document\}

\end{document}
